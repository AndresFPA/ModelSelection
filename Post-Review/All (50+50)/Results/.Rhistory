aes(x     = extrav,
y     = popular,
col   = class,
group = class))+ #to add the colours for different classes
geom_point(size     = 1.2,
alpha    = .8,
position = "jitter")+ #to add some random noise for plotting purposes
theme_minimal()+
theme(legend.position = "none")+
scale_color_gradientn(colours = rainbow(100))+
geom_smooth(method = lm,
se     = FALSE,
size   = .5,
alpha  = .8)+ # to add regression line
labs(title    = "Popularity vs. Extraversion",
subtitle = "add colours for different classes and regression lines")
# Which classes are more extreme?
# Create function for this
f1 <- function(data, x, y, grouping, n.highest = 3, n.lowest = 3){
groupinglevel <- data[,grouping]
res           <- data.frame(coef = rep(NA, length(unique(groupinglevel))), group = unique(groupinglevel))
names(res)    <- c("coef", grouping)
for(i in 1:length(unique(groupinglevel))){
data2    <- as.data.frame(data[data[,grouping] == i,])
res[i,1] <- as.numeric(lm(data2[, y] ~ data2[, x])$coefficients[2])
}
top    <- res %>% top_n(n.highest, coef)
bottom <- res %>% top_n(-n.lowest, coef)
res    <- res %>% mutate(high_and_low = ifelse(coef %in% top$coef, "top",  ifelse(coef %in% bottom$coef, "bottom", "none")))
data3  <- left_join(data, res)
return(data3)
}
# Use the function and highlight the more extreme groups
f1(data = as.data.frame(popular2data),
x    = "extrav",
y    = "popular",
grouping = "class",
n.highest = 3,
n.lowest = 3) %>%
ggplot()+
geom_point(aes(x     = extrav,
y     = popular,
fill  = class,
group = class),
size     =  1,
alpha    = .5,
position = "jitter",
shape    = 21,
col      = "white")+
geom_smooth(aes(x     = extrav,
y     = popular,
col   = high_and_low,
group = class,
size  = as.factor(high_and_low),
alpha = as.factor(high_and_low)),
method = lm,
se     = FALSE)+
theme_minimal()+
theme(legend.position = "none")+
scale_fill_gradientn(colours = rainbow(100))+
scale_color_manual(values=c("top"      = "blue",
"bottom"   = "red",
"none"     = "grey40"))+
scale_size_manual(values=c("top"       = 1.2,
"bottom"   = 1.2,
"none"     = .5))+
scale_alpha_manual(values=c("top"      = 1,
"bottom"    = 1,
"none"      =.3))+
labs(title="Linear Relationship Between Popularity and Extraversion for 100 Classes",
subtitle="The 6 with the most extreme relationship have been highlighted red and blue")
##### ----------------------------------------------------------------------------------------------
# Start estimating bayesian multilevel models
# First, intercept only
interceptonlymodeltest <- brm(popular ~ 1 + (1 | class),
data   = popular2data,
warmup = 100,
iter   = 200,
chains = 2,
init  = "random",
cores  = 2)  #the cores function tells STAN to make use of 2 CPU cores simultaneously instead of just 1.
summary(interceptonlymodeltest)
# Add more iterations
interceptonlymodel <- brm(popular ~ 1 + (1|class),
data = popular2data,
warmup = 1000, iter = 3000,
cores = 2, chains = 2,
seed = 123) #to run the model
summary(interceptonlymodel)
hyp <- "sd_class__Intercept^2 / (sd_class__Intercept^2 + sigma^2) = 0"
hypothesis(interceptonlymodel, hyp, class = NULL)
# Second, add the other first-level predictors (individual level)
model1 <- brm(popular ~ 1 + sex + extrav + (1|class),
data = popular2data,
warmup = 1000, iter = 3000,
cores = 2, chains = 2,
seed = 123) #to run the model
summary(model1)
model1tranformed <- ggs(model1) # cannot use
View(model1tranformed)
mcmc_plot(model1, type = "trace")
ggplot(filter(model1tranformed, Parameter %in% c("b_Intercept", "b_extrav", "b_sex")),
aes(x   = Iteration,
y   = value,
col = as.factor(Chain)))+
geom_line() +
geom_vline(xintercept = 1000)+
facet_grid(Parameter ~ . ,
scale  = 'free_y',
switch = 'y')+
labs(title = "Caterpillar Plots",
col   = "Chains")
# Check credible intervals
# Not working until ggs works
ggplot(filter(model1tranformed,
Parameter == "b_Intercept",
Iteration > 1000),
aes(x = value))+
geom_density(fill  = "yellow",
alpha = .5)+
geom_vline(xintercept = 0,
color  = "red",
size = 1)+
scale_x_continuous(name   = "Value",
limits = c(-1, 3)) +
geom_vline(xintercept = summary(model1)$fixed[1,3],
color = "blue",
linetype = 2) +
geom_vline(xintercept = summary(model1)$fixed[1,4],
color = "blue",
linetype = 2) +
theme_light() +
labs(title = "Posterior Density of Intercept")
ggplot(filter(model1tranformed, Parameter == "b_extrav", Iteration > 1000), aes(x = value))+
geom_density(fill = "orange", alpha = .5)+
geom_vline(xintercept = 0, col = "red", size = 1)+
scale_x_continuous(name = "Value", limits = c(-.2, .6))+
geom_vline(xintercept = summary(model1)$fixed[3,3], col = "blue", linetype = 2)+
geom_vline(xintercept = summary(model1)$fixed[3,4], col = "blue", linetype = 2)+
theme_light()+
labs(title = "Posterior Density of Regression Coefficient for Extraversion")
ggplot(filter(model1tranformed, Parameter == "b_sex", Iteration > 1000), aes(x = value))+
geom_density(fill = "red", alpha = .5)+
geom_vline(xintercept = 0, col = "red", size = 1)+
scale_x_continuous(name = "Value", limits = c(-.2, 1.5))+
geom_vline(xintercept = summary(model1)$fixed[2,3], col = "blue", linetype = 2)+
geom_vline(xintercept = summary(model1)$fixed[2,4], col = "blue", linetype = 2)+
theme_light()+
labs(title = "Posterior Density of Regression Coefficient for Sex")
summary(model1)$fixed
install.packages("tidybayes")
install.packages("devtools")
install.packages("fpp3")
install.packages("lavaan")
# Load required packages
library(tidyverse)
library(brms)
library(bayesplot)
# Generate simulated data
set.seed(23)
n <- 100
temperature <- runif(n, min = 0, max = 30) # Random temperatures between 0 and 30
intercept <- 10
slope <- 0.5
noise_sd <- 3
# round to whole numbers
bike_thefts <- (intercept + slope * temperature + rnorm(n, sd = noise_sd)) %>%
round(., 0)
data <- tibble(temperature = temperature, bike_thefts = bike_thefts)
View(data)
# Some initial descriptions
summary(data)
head(data)
library(ggplot2)
ggplot(data, aes(x = temperature, y = bike_thefts)) +
geom_point() +
labs(x = "Temperature", y = "Bike thefts") +
theme_minimal()
# 2) Specify and fit the model, note the sample_prior = "only"
# this makes sure we are going to look at the prior predictive samples.
fit <- brm(bike_thefts ~ temperature, data = data,
prior = priors, family = gaussian(),
sample_prior = "only", seed = 555)
# --------------------------------------------------------------------------------------------------
# Start Bayes analysis
# 1) set the priors
priors <- prior(normal(10, 5), class = "Intercept") +
prior(normal(0, 1), class = "b")
# 2) Specify and fit the model, note the sample_prior = "only"
# this makes sure we are going to look at the prior predictive samples.
fit <- brm(bike_thefts ~ temperature, data = data,
prior = priors, family = gaussian(),
sample_prior = "only", seed = 555)
library(rstantools)
# 2) Specify and fit the model, note the sample_prior = "only"
# this makes sure we are going to look at the prior predictive samples.
fit <- brm(bike_thefts ~ temperature, data = data,
prior = priors, family = gaussian(),
sample_prior = "only", seed = 555)
pp_check(fit, type = "dens_overlay", prefix = "ppd", ndraws = 100)
# 2) Specify and fit the model, note the sample_prior = "only"
# this makes sure we are going to look at the prior predictive samples.
fit <- brm(bike_thefts ~ temperature, data = data,
prior = priors, family = gaussian(),
sample_prior = "only", seed = 555)
sqrt(0.1)
load("~/GitHub/OrdinalSim/Results/Times/CatTimeIgnRow87Rep1.Rdata")
ctime.ign.cat
1102.64/60
load("~/GitHub/OrdinalSim/Results/Times/CatTimeRow87Rep1.Rdata")
ctime.cat
426.31/60
20*17400
46107-44343
?lavaan
?lavaan::lavaan
data <- HolzingerSwineford1939
library(lavaan)
# The Holzinger and Swineford (1939) example
HS.model <- ' visual  =~ x1 + x2 + x3
textual =~ x4 + x5 + x6
speed   =~ x7 + x8 + x9 '
data <- HolzingerSwineford1939
View(data)
data$x1 <- ordered(as.numeric(cut(data$x1, breaks = c(-Inf, c(3, 5), Inf))))
data$x2 <- ordered(as.numeric(cut(data$x2, breaks = c(-Inf, c(3, 5), Inf))))
data$x3 <- ordered(as.numeric(cut(data$x3, breaks = c(-Inf, c(4), Inf))))
View(data)
# The Holzinger and Swineford (1939) example
HS.model <- ' visual  =~ x1 + x2 + x3 '
fit <- sem(HS.model, data=data, ordered = T)
summary(fit, fit.measures=TRUE)
load("C:/Users/perezalo/Documents/GitHub/ModelSelection_Simulation/Pre-Review/Results/Result_IgnRowARI8.Rdata")
View(ResultsRowARI_ignored)
load("C:/Users/perezalo/Documents/GitHub/ModelSelection_Simulation/Pre-Review/Results/ResultRow140.Rdata")
View(ResultsRow)
?lavaan
?lavaan::lavaan
# The Holzinger and Swineford (1939) example
HS.model <- ' visual  =~ x1 + x2 + x3
textual =~ x4 + x5 + x6
speed   =~ x7 + x8 + x9 '
fit <- cfa(HS.model, data=HolzingerSwineford1939, std.lv = T)
library(lavaan)
fit <- cfa(HS.model, data=HolzingerSwineford1939, std.lv = T, group = "school")
fit.unstd <- cfa(HS.model, data=HolzingerSwineford1939, std.lv = F, group = "school")
lapply(X = lavInspect(fit, what = "est"), FUN = "[[")
lavInspect(fit, what = "est")
lapply(X = lavInspect(fit, what = "est"), "psi", "[[")
lapply(X = lavInspect(fit, what = "est", "psi"), , "[[")
lapply(X = lavInspect(fit, what = "est", "psi"), "[[")
lavInspect(fit, what = "est")
lapply(lavInspect(fit, what = "est"), "[[", "psi")
fit       <- cfa(HS.model, data=HolzingerSwineford1939, std.lv = T, group = "school", group.equal = "loadings")
fit.unstd <- cfa(HS.model, data=HolzingerSwineford1939, std.lv = F, group = "school", group.equal = "loadings")
lapply(lavInspect(fit, what = "est"), "[[", "psi")
lapply(lavInspect(fit.unstd, what = "est"), "[[", "psi")
# Rescale variances of "fit"
lambda <- lapply(lavInspect(fit, what = "est"), "[[", "lambda")
lambda
cov_eta <- lapply(lavInspect(fit, what = "est"), "[[", "psi")
# First loading of each factor
# First group
loadings <- apply(lambda[[1]], 2, function(x) {x[which(x != 0)]})[1, ]
loadings
sds <- sqrt(diag(cov_eta[[1]]) * loadings^2)
sds
diag(cov_eta[[1]]
)
lavaan::cor2cov(R = cov_eta[[1]], sds = sds)
lapply(lavInspect(fit.unstd, what = "est"), "[[", "psi")[[1]] # Compare to the unstd one
# Second group
loadings <- apply(lambda[[2]], 2, function(x) {x[which(x != 0)]})[1, ]
sds <- sqrt(diag(cov_eta[[2]]) * loadings^2)
# Rescale second group
lavaan::cor2cov(R = cov_eta[[2]], sds = sds)
lapply(lavInspect(fit.unstd, what = "est"), "[[", "psi")[[1]] # Compare to the unstd one
# First, get them to be correlations
cov_eta[[2]] <- stats::cov2cor(cov_eta[[2]])
cov_eta[[2]]
lavaan::cor2cov(R = cov_eta[[2]], sds = sds)
lapply(lavInspect(fit.unstd, what = "est"), "[[", "psi")[[1]] # Compare to the unstd one
lapply(lavInspect(fit.unstd, what = "est"), "[[", "psi")[[2]] # Compare to the unstd one
# The Holzinger and Swineford (1939) example
HS.model <- ' visual  =~ x1 + x2 + x3
textual =~ x4 + x5 + x6
speed   =~ x7 + x8 + x9 '
fit       <- cfa(HS.model, data=HolzingerSwineford1939, std.lv = T, group = "school", group.equal = "loadings")
fit.unstd <- cfa(HS.model, data=HolzingerSwineford1939, std.lv = F, group = "school", group.equal = "loadings")
# Group 2 variances are different
lapply(lavInspect(fit, what = "est"), "[[", "psi")
lapply(lavInspect(fit.unstd, what = "est"), "[[", "psi")
# Rescale variances of "fit"
# Extract necessary objects
lambda <- lapply(lavInspect(fit, what = "est"), "[[", "lambda")
cov_eta <- lapply(lavInspect(fit, what = "est"), "[[", "psi")
# First loading of each factor
# First group
loadings <- apply(lambda[[1]], 2, function(x) {x[which(x != 0)]})[1, ]
sds <- sqrt(diag(cov_eta[[1]]) * loadings^2)
# Rescaled first group
lavaan::cor2cov(R = cov_eta[[1]], sds = sds)
lapply(lavInspect(fit.unstd, what = "est"), "[[", "psi")[[1]] # Compare to the unstd one
# Second group
loadings <- apply(lambda[[2]], 2, function(x) {x[which(x != 0)]})[1, ]
sds <- sqrt(diag(cov_eta[[2]]) * loadings^2)
# Rescale second group # INCORRECT :(
lavaan::cor2cov(R = cov_eta[[2]], sds = sds)
lapply(lavInspect(fit.unstd, what = "est"), "[[", "psi")[[2]] # Compare to the unstd one
# First, get them to be correlations
cov_eta[[2]] <- stats::cov2cor(cov_eta[[2]])
lavaan::cor2cov(R = cov_eta[[2]], sds = sds)
lapply(lavInspect(fit.unstd, what = "est"), "[[", "psi")[[2]] # Compare to the unstd one
sort(c(5,4,6))
sort(c(5,4,6), decreasing = T)
sort(c(5,4,6), decreasing = T)[2]
which(c(5,4,6) == sort(c(5,4,6), decreasing = T)[2])
1/0.7
0.7*1.42
lapply(lavInspect(fit, what = "est"), "[[", "lambda")
1/0.897
1.114827*0.537
lapply(lavInspect(fit.unstd, what = "est"), "[[", "lambda")[[2]]
0.897/0.897
0.537/0.897
Rfast::nth(x = c(2,5,6,4,1), k = 2, index.return = T) == 2
Rfast::nth(x = c(2,5,6,4,1), k = 2, index.return = T)
Rfast::nth(x = t(c(2,5,6,4,1)), k = 2, index.return = T)
Rfast::nth(x = t(t(c(2,5,6,4,1))), k = 2, index.return = T)
t(t(c(2,5,6,4,1)))
Rfast::nth(x = t(t(c(2,5,6,4,1))), k = 2, index.return = F)
Rfast::nth(x = t(t(c(2,5,6,4,1))), k = 2, index.return = T, num.of.nths = 2)
Rfast::nth(x = t(t(c(2,5,6,4,1))), k = 2, index.return = F, num.of.nths = 2)
Rfast::nth(x = t(t(c(2,5,6,4,1))), k = 2, index.return = T, num.of.nths = 2) %in% 2
Rfast::nth(x = t(t(c(2,5,6,4,1))), k = 2, index.return = F, num.of.nths = 2) %in% 2
any(Rfast::nth(x = t(t(c(2,5,6,4,1))), k = 2, index.return = F, num.of.nths = 2) %in% 2
)
any(Rfast::nth(x = t(t(c(2,5,6,4,1))), k = 2, index.return = F, num.of.nths = 2) %in% 2)
source("~/GitHub/MMG-SEM/R/MMG-SEM.R", echo=TRUE)
source("~/GitHub/MMG-SEM/R/MMG-SEM.R", echo=TRUE)
library(lavaan)
library(qwraps2)
library(tidyr)
library(dplyr)
library(xtable)
library(ggpubr)
library(ggplot2)
library(ggthemes)
# library(Cairo)
# CairoWin()
setwd("C:/Users/perezalo/OneDrive - Tilburg University/1. Papers/Paper 2/Methodology (Journal)/Review/Simulation results/ModelSelection_Simulation/Post-Review/All (50+50)/Results")
# Load empty final results matrix
load("FinalResults.Rdata")
colnames(Results_final)[1:13] <- c("entropyR2", "Chull", "BIC_G", "BIC_N", "AIC", "AIC3", "ICL",
"Chull_fac", "BIC_G_fac", "BIC_N_fac", "AIC_fac", "AIC3_fac", "ICL_fac")
load("design.Rdata")
# Merge datasets
design$Condition <- as.numeric(rownames(design))
Results_final <- merge(x = design, y = Results_final, by = "Condition")
col_order <- c("Condition", "Replication", "nclus", "ngroups", "coeff", "N_g", "balance", "sd",
"entropyR2", "Chull", "BIC_G", "BIC_N", "AIC", "AIC3", "ICL",
"Chull_fac", "BIC_G_fac", "BIC_N_fac", "AIC_fac", "AIC3_fac", "ICL_fac")
Results_final <- Results_final[, col_order]
rm(col_order)
# Fill in the matrix with all results
ncond <- unique(Results_final$Condition) # How many conditions?
K <- length(unique(Results_final$Replication)) # How many replications?
for (i in ncond) {
test <- NA
test <- try(load(paste0("ResultRow", i, ".Rdata")))
if(!c(class(test) == "try-error")){
Results_final[(K*(i-1)+1):(i*K), 9:21] <- ResultsRow
}
}
# remove uncomplete entries
# Results_final <- Results_final[!is.na(Results_final$BIC_G), ]
# Turn NAs from Chull into FALSE input (Chull was not able to select any model)
# apply(X = apply(X = Results_final, MARGIN = 2, FUN = is.na), MARGIN = 2, FUN = sum)
# Results_final$`Chull Scree` <- ifelse(test = is.na(Results_final$`Chull Scree`), yes = FALSE, no = Results_final$`Chull Scree`)
# REMEMBER THAT THE RESULTS MUST TYPE "FACTOR"
measures <- Results_final %>% dplyr::select(Chull:ICL_fac) %>% as.matrix() %>% as.data.frame()
measures <- lapply(X = measures, FUN = factor, levels = c("-1", "0", "1"), labels = c("-1", "0", "1")) %>% as.data.frame()
Results_final[, 10:21] <- measures
####################################################################################################
############################ TABLES - CLUSTER AND PARAMETER RECOVERY ###############################
####################################################################################################
# Check mean results per simulation factor
# Create a function for this
count_results <- function(data, by, type = "count"){
# Extract necessary columns
reduced <- data %>% dplyr::select(Chull:ICL_fac)
#Initialize objects to store
counted        <- vector(mode = "list", length = ncol(reduced))
names(counted) <- colnames(reduced)
final <- c()
# browser()
# Count per column
for(i in 1:ncol(reduced)){
if(length(by) == 1 && by == "total"){
counted[[i]] <- data %>% count(get(colnames(reduced)[i]), .drop = F) %>% filter(!is.na(`get(colnames(reduced)[i])`)) # Count and remove NA
if(type == "relative"){
counted[[i]][, ncol(counted[[i]])] <- round(counted[[i]][, ncol(counted[[i]]), drop = F]/sum(counted[[i]][, ncol(counted[[i]]), drop = F]), 3)
}
# browser()
colnames(counted[[i]]) <- c("result", colnames(reduced)[i]) # change colnames
ifelse(test = i == 1, yes = final <- counted[[i]][, 1, drop = F], no = final <- final) # for the first iteration, keep the group variable and results column
final <- cbind(final, counted[[i]][, ncol(counted[[i]]), drop = F]) # Add the results for each measure
} else {
# browser()
counted[[i]] <- data %>% group_by(across(all_of(by))) %>% count(get(colnames(reduced)[i]), .drop = F) %>% filter(!is.na(`get(colnames(reduced)[i])`)) # count per measure
# browser()
if(type == "relative"){
counted[[i]] <- counted[[i]] %>% group_by(across(all_of(by))) %>% mutate(prop_n = n / sum(n)) %>% select(!n)
# counted[[i]][, ncol(counted[[i]])] <- round(counted[[i]][, ncol(counted[[i]])]/sum(counted[[i]][1:3, ncol(counted[[i]])],), 3)
}
colnames(counted[[i]]) <- c(by, "result", colnames(reduced)[i]) # change colnames
ifelse(test = i == 1, yes = final <- counted[[i]][, c(seq_len(length(by)), (length(by) + 1))], no = final <- final) # for the first iteration, keep the group variable and results column
final <- cbind(final, counted[[i]][, ncol(counted[[i]]), drop = F]) # Add the results for each measure
}
}
return(final)
}
# Pre-check to know if there are NAs
colSums(apply(Results_final, 2, is.na))
a <- count_results(data = Results_final, by = "total", type = "relative") %>% filter(result == "0")
a
a
a1 <- a %>% dplyr::select(Chull:ICL) %>% pivot_longer(cols = Chull:ICL, names_to = "Measure", values_to = "Proportion")
a1
a1$Proportion <- round(a1$Proportion, 3)
a1$Measure <- factor(a1$Measure, levels = c("BIC_N", "ICL", "BIC_G", "AIC3", "AIC", "Chull"))
plot <- ggplot(data = a1, aes(x = Proportion, y = Measure)) +
geom_bar(stat = 'identity', fill = 'skyblue') +
labs(title = 'Ranking of Best-Performing Models', x = 'Performance', y = 'Model Selection measures') +
# labs(x = expression("Within-cluster differences (" * sigma[beta] * ")"),  # Combines text with Greek letter, no space
#      y = expression("Model Selection measure")) +
scale_y_discrete(labels = c("Chull" = "CHull",
"AIC" = "AIC",
"AIC3" = expression(AIC[3]),
"BIC_G" = expression(BIC[G]),
"ICL" = "ICL",
"BIC_N" = expression(BIC[N])))
plot
plot <- ggplot(data = a1, aes(x = Proportion, y = Measure)) +
geom_bar(stat = 'identity', fill = 'skyblue') +
labs(title = 'Ranking of Best-Performing Models', x = 'Performance', y = 'Model Selection measures') +
geom_text(aes(label = Proportion), hjust = -0.1, color = 'black') +
# labs(x = expression("Within-cluster differences (" * sigma[beta] * ")"),  # Combines text with Greek letter, no space
#      y = expression("Model Selection measure")) +
scale_y_discrete(labels = c("Chull" = "CHull",
"AIC" = "AIC",
"AIC3" = expression(AIC[3]),
"BIC_G" = expression(BIC[G]),
"ICL" = "ICL",
"BIC_N" = expression(BIC[N])))
plot
plot <- ggplot(data = a1, aes(x = Proportion, y = Measure)) +
geom_bar(stat = 'identity', fill = 'skyblue') +
labs(title = 'Ranking of Best-Performing Models', x = 'Performance', y = 'Model Selection measures') +
geom_text(aes(label = Proportion), hjust = -0.1, color = 'black') +
theme(plot.title = element_text(hjust = 0.5)) +
# labs(x = expression("Within-cluster differences (" * sigma[beta] * ")"),  # Combines text with Greek letter, no space
#      y = expression("Model Selection measure")) +
scale_y_discrete(labels = c("Chull" = "CHull",
"AIC" = "AIC",
"AIC3" = expression(AIC[3]),
"BIC_G" = expression(BIC[G]),
"ICL" = "ICL",
"BIC_N" = expression(BIC[N])))
plot
plot <- ggplot(data = a1, aes(x = Proportion, y = Measure)) +
geom_bar(stat = 'identity', fill = 'skyblue') +
labs(title = 'Ranking of Best-Performing Models', x = 'Performance', y = 'Model Selection measures') +
geom_text(aes(label = Proportion), hjust = 0.5, color = 'black') +
theme(plot.title = element_text(hjust = 0.5)) +
# labs(x = expression("Within-cluster differences (" * sigma[beta] * ")"),  # Combines text with Greek letter, no space
#      y = expression("Model Selection measure")) +
scale_y_discrete(labels = c("Chull" = "CHull",
"AIC" = "AIC",
"AIC3" = expression(AIC[3]),
"BIC_G" = expression(BIC[G]),
"ICL" = "ICL",
"BIC_N" = expression(BIC[N])))
plot
plot <- ggplot(data = a1, aes(x = Proportion, y = Measure)) +
geom_bar(stat = 'identity', fill = 'skyblue') +
labs(title = 'Ranking of Best-Performing Models', x = 'Performance', y = 'Model Selection measures') +
geom_text(aes(label = Proportion), hjust = 0.9, color = 'black') +
theme(plot.title = element_text(hjust = 0.5)) +
# labs(x = expression("Within-cluster differences (" * sigma[beta] * ")"),  # Combines text with Greek letter, no space
#      y = expression("Model Selection measure")) +
scale_y_discrete(labels = c("Chull" = "CHull",
"AIC" = "AIC",
"AIC3" = expression(AIC[3]),
"BIC_G" = expression(BIC[G]),
"ICL" = "ICL",
"BIC_N" = expression(BIC[N])))
plot
plot <- ggplot(data = a1, aes(x = Proportion, y = Measure)) +
geom_bar(stat = 'identity', fill = 'skyblue') +
labs(title = 'Ranking of Best-Performing Models', x = 'Performance', y = 'Model Selection measures') +
geom_text(aes(label = Proportion), hjust = 1.5, color = 'black') +
theme(plot.title = element_text(hjust = 0.5)) +
# labs(x = expression("Within-cluster differences (" * sigma[beta] * ")"),  # Combines text with Greek letter, no space
#      y = expression("Model Selection measure")) +
scale_y_discrete(labels = c("Chull" = "CHull",
"AIC" = "AIC",
"AIC3" = expression(AIC[3]),
"BIC_G" = expression(BIC[G]),
"ICL" = "ICL",
"BIC_N" = expression(BIC[N])))
plot
