library(haven) # to load the SPSS .sav file
library(RColorBrewer) # needed for some extra colours in one of the graphs
library(ggmcmc)
library(ggthemes)
library(ggridges)
# Load the data
popular2data <- read_sav(file = "https://github.com/MultiLevelAnalysis/Datasets-third-edition-Multilevel-book/blob/master/chapter%202/popularity/SPSS/popular2.sav?raw=true")
# Select relevant variables
popular2data <- select(popular2data, pupil, class, extrav, sex, texp, popular) # we select just the variables we will use
head(popular2data) # we have a look at the first 6 observations
# Visually check some relations
ggplot(data  = popular2data,
aes(x = extrav,
y = popular))+
geom_point(size     = 1.2,
alpha    = .8,
position = "jitter")+ #to add some random noise for plotting purposes
geom_smooth(method = lm,
se     = FALSE,
col    = "black",
linewidth   = .5,
alpha  = .8)+ # to add regression line
theme_minimal()+
labs(title    = "Popularity vs. Extraversion",
subtitle = "add regression line")
# Do it again, but different colors for each class (multilevel stuff)
ggplot(data      = popular2data,
aes(x     = extrav,
y     = popular,
col   = class,
group = class))+ #to add the colours for different classes
geom_point(size     = 1.2,
alpha    = .8,
position = "jitter")+ #to add some random noise for plotting purposes
theme_minimal()+
theme(legend.position = "none")+
scale_color_gradientn(colours = rainbow(100))+
geom_smooth(method = lm,
se     = FALSE,
size   = .5,
alpha  = .8)+ # to add regression line
labs(title    = "Popularity vs. Extraversion",
subtitle = "add colours for different classes and regression lines")
# Which classes are more extreme?
# Create function for this
f1 <- function(data, x, y, grouping, n.highest = 3, n.lowest = 3){
groupinglevel <- data[,grouping]
res           <- data.frame(coef = rep(NA, length(unique(groupinglevel))), group = unique(groupinglevel))
names(res)    <- c("coef", grouping)
for(i in 1:length(unique(groupinglevel))){
data2    <- as.data.frame(data[data[,grouping] == i,])
res[i,1] <- as.numeric(lm(data2[, y] ~ data2[, x])$coefficients[2])
}
top    <- res %>% top_n(n.highest, coef)
bottom <- res %>% top_n(-n.lowest, coef)
res    <- res %>% mutate(high_and_low = ifelse(coef %in% top$coef, "top",  ifelse(coef %in% bottom$coef, "bottom", "none")))
data3  <- left_join(data, res)
return(data3)
}
# Use the function and highlight the more extreme groups
f1(data = as.data.frame(popular2data),
x    = "extrav",
y    = "popular",
grouping = "class",
n.highest = 3,
n.lowest = 3) %>%
ggplot()+
geom_point(aes(x     = extrav,
y     = popular,
fill  = class,
group = class),
size     =  1,
alpha    = .5,
position = "jitter",
shape    = 21,
col      = "white")+
geom_smooth(aes(x     = extrav,
y     = popular,
col   = high_and_low,
group = class,
size  = as.factor(high_and_low),
alpha = as.factor(high_and_low)),
method = lm,
se     = FALSE)+
theme_minimal()+
theme(legend.position = "none")+
scale_fill_gradientn(colours = rainbow(100))+
scale_color_manual(values=c("top"      = "blue",
"bottom"   = "red",
"none"     = "grey40"))+
scale_size_manual(values=c("top"       = 1.2,
"bottom"   = 1.2,
"none"     = .5))+
scale_alpha_manual(values=c("top"      = 1,
"bottom"    = 1,
"none"      =.3))+
labs(title="Linear Relationship Between Popularity and Extraversion for 100 Classes",
subtitle="The 6 with the most extreme relationship have been highlighted red and blue")
##### ----------------------------------------------------------------------------------------------
# Start estimating bayesian multilevel models
# First, intercept only
interceptonlymodeltest <- brm(popular ~ 1 + (1 | class),
data   = popular2data,
warmup = 100,
iter   = 200,
chains = 2,
init  = "random",
cores  = 2)  #the cores function tells STAN to make use of 2 CPU cores simultaneously instead of just 1.
summary(interceptonlymodeltest)
# Add more iterations
interceptonlymodel <- brm(popular ~ 1 + (1|class),
data = popular2data,
warmup = 1000, iter = 3000,
cores = 2, chains = 2,
seed = 123) #to run the model
summary(interceptonlymodel)
hyp <- "sd_class__Intercept^2 / (sd_class__Intercept^2 + sigma^2) = 0"
hypothesis(interceptonlymodel, hyp, class = NULL)
# Second, add the other first-level predictors (individual level)
model1 <- brm(popular ~ 1 + sex + extrav + (1|class),
data = popular2data,
warmup = 1000, iter = 3000,
cores = 2, chains = 2,
seed = 123) #to run the model
summary(model1)
model1tranformed <- ggs(model1) # cannot use
View(model1tranformed)
mcmc_plot(model1, type = "trace")
ggplot(filter(model1tranformed, Parameter %in% c("b_Intercept", "b_extrav", "b_sex")),
aes(x   = Iteration,
y   = value,
col = as.factor(Chain)))+
geom_line() +
geom_vline(xintercept = 1000)+
facet_grid(Parameter ~ . ,
scale  = 'free_y',
switch = 'y')+
labs(title = "Caterpillar Plots",
col   = "Chains")
# Check credible intervals
# Not working until ggs works
ggplot(filter(model1tranformed,
Parameter == "b_Intercept",
Iteration > 1000),
aes(x = value))+
geom_density(fill  = "yellow",
alpha = .5)+
geom_vline(xintercept = 0,
color  = "red",
size = 1)+
scale_x_continuous(name   = "Value",
limits = c(-1, 3)) +
geom_vline(xintercept = summary(model1)$fixed[1,3],
color = "blue",
linetype = 2) +
geom_vline(xintercept = summary(model1)$fixed[1,4],
color = "blue",
linetype = 2) +
theme_light() +
labs(title = "Posterior Density of Intercept")
ggplot(filter(model1tranformed, Parameter == "b_extrav", Iteration > 1000), aes(x = value))+
geom_density(fill = "orange", alpha = .5)+
geom_vline(xintercept = 0, col = "red", size = 1)+
scale_x_continuous(name = "Value", limits = c(-.2, .6))+
geom_vline(xintercept = summary(model1)$fixed[3,3], col = "blue", linetype = 2)+
geom_vline(xintercept = summary(model1)$fixed[3,4], col = "blue", linetype = 2)+
theme_light()+
labs(title = "Posterior Density of Regression Coefficient for Extraversion")
ggplot(filter(model1tranformed, Parameter == "b_sex", Iteration > 1000), aes(x = value))+
geom_density(fill = "red", alpha = .5)+
geom_vline(xintercept = 0, col = "red", size = 1)+
scale_x_continuous(name = "Value", limits = c(-.2, 1.5))+
geom_vline(xintercept = summary(model1)$fixed[2,3], col = "blue", linetype = 2)+
geom_vline(xintercept = summary(model1)$fixed[2,4], col = "blue", linetype = 2)+
theme_light()+
labs(title = "Posterior Density of Regression Coefficient for Sex")
summary(model1)$fixed
install.packages("tidybayes")
install.packages("devtools")
install.packages("fpp3")
install.packages("lavaan")
# Load required packages
library(tidyverse)
library(brms)
library(bayesplot)
# Generate simulated data
set.seed(23)
n <- 100
temperature <- runif(n, min = 0, max = 30) # Random temperatures between 0 and 30
intercept <- 10
slope <- 0.5
noise_sd <- 3
# round to whole numbers
bike_thefts <- (intercept + slope * temperature + rnorm(n, sd = noise_sd)) %>%
round(., 0)
data <- tibble(temperature = temperature, bike_thefts = bike_thefts)
View(data)
# Some initial descriptions
summary(data)
head(data)
library(ggplot2)
ggplot(data, aes(x = temperature, y = bike_thefts)) +
geom_point() +
labs(x = "Temperature", y = "Bike thefts") +
theme_minimal()
# 2) Specify and fit the model, note the sample_prior = "only"
# this makes sure we are going to look at the prior predictive samples.
fit <- brm(bike_thefts ~ temperature, data = data,
prior = priors, family = gaussian(),
sample_prior = "only", seed = 555)
# --------------------------------------------------------------------------------------------------
# Start Bayes analysis
# 1) set the priors
priors <- prior(normal(10, 5), class = "Intercept") +
prior(normal(0, 1), class = "b")
# 2) Specify and fit the model, note the sample_prior = "only"
# this makes sure we are going to look at the prior predictive samples.
fit <- brm(bike_thefts ~ temperature, data = data,
prior = priors, family = gaussian(),
sample_prior = "only", seed = 555)
library(rstantools)
# 2) Specify and fit the model, note the sample_prior = "only"
# this makes sure we are going to look at the prior predictive samples.
fit <- brm(bike_thefts ~ temperature, data = data,
prior = priors, family = gaussian(),
sample_prior = "only", seed = 555)
pp_check(fit, type = "dens_overlay", prefix = "ppd", ndraws = 100)
# 2) Specify and fit the model, note the sample_prior = "only"
# this makes sure we are going to look at the prior predictive samples.
fit <- brm(bike_thefts ~ temperature, data = data,
prior = priors, family = gaussian(),
sample_prior = "only", seed = 555)
sqrt(0.1)
load("~/GitHub/OrdinalSim/Results/Times/CatTimeIgnRow87Rep1.Rdata")
ctime.ign.cat
1102.64/60
load("~/GitHub/OrdinalSim/Results/Times/CatTimeRow87Rep1.Rdata")
ctime.cat
426.31/60
20*17400
46107-44343
?lavaan
?lavaan::lavaan
data <- HolzingerSwineford1939
library(lavaan)
# The Holzinger and Swineford (1939) example
HS.model <- ' visual  =~ x1 + x2 + x3
textual =~ x4 + x5 + x6
speed   =~ x7 + x8 + x9 '
data <- HolzingerSwineford1939
View(data)
data$x1 <- ordered(as.numeric(cut(data$x1, breaks = c(-Inf, c(3, 5), Inf))))
data$x2 <- ordered(as.numeric(cut(data$x2, breaks = c(-Inf, c(3, 5), Inf))))
data$x3 <- ordered(as.numeric(cut(data$x3, breaks = c(-Inf, c(4), Inf))))
View(data)
# The Holzinger and Swineford (1939) example
HS.model <- ' visual  =~ x1 + x2 + x3 '
fit <- sem(HS.model, data=data, ordered = T)
summary(fit, fit.measures=TRUE)
library(lavaan)
library(qwraps2)
library(fpp3)
library(dplyr)
library(xtable)
library(ggpubr)
library(ggplot2)
library(ggthemes)
# library(Cairo)
# CairoWin()
# Set wd
setwd("~/GitHub/ModelSelection_Simulation/Post-Review/K1/Results")
# Load empty final results matrix
load("FinalResults.Rdata")
colnames(Results_final)[1:13] <- c("entropyR2", "Chull", "BIC_G", "BIC_N", "AIC", "AIC3", "ICL",
"Chull_fac", "BIC_G_fac", "BIC_N_fac", "AIC_fac", "AIC3_fac", "ICL_fac")
load("design.Rdata")
# Merge datasets
design$Condition <- as.numeric(rownames(design))
Results_final <- merge(x = design, y = Results_final, by = "Condition")
col_order <- c("Condition", "Replication", "nclus", "ngroups", "coeff", "N_g", "balance", "sd",
"entropyR2", "Chull", "BIC_G", "BIC_N", "AIC", "AIC3", "ICL",
"Chull_fac", "BIC_G_fac", "BIC_N_fac", "AIC_fac", "AIC3_fac", "ICL_fac")
Results_final <- Results_final[, col_order]
rm(col_order)
# Fill in the matrix with all results
ncond <- unique(Results_final$Condition) # How many conditions?
K <- length(unique(Results_final$Replication)) # How many replications?
for (i in ncond) {
test <- NA
test <- try(load(paste0("ResultRow", i, ".Rdata")))
if(!c(class(test) == "try-error")){
Results_final[(K*(i-1)+1):(i*K), 9:21] <- ResultsRow
}
}
# REMEMBER THAT THE RESULTS MUST TYPE "FACTOR"
measures <- Results_final %>% dplyr::select(Chull:ICL_fac) %>% as.matrix() %>% as.data.frame()
measures <- lapply(X = measures, FUN = factor, levels = c("-1", "0", "1"), labels = c("-1", "0", "1")) %>% as.data.frame()
Results_final[, 10:21] <- measures
# Check mean results per simulation factor
# Create a function for this
count_results <- function(data, by, type = "count"){
# Extract necessary columns
reduced <- data %>% dplyr::select(Chull:ICL_fac)
#Initialize objects to store
counted        <- vector(mode = "list", length = ncol(reduced))
names(counted) <- colnames(reduced)
final <- c()
# browser()
# Count per column
for(i in 1:ncol(reduced)){
if(length(by) == 1 && by == "total"){
counted[[i]] <- data %>% count(get(colnames(reduced)[i]), .drop = F) %>% filter(!is.na(`get(colnames(reduced)[i])`)) # Count and remove NA
if(type == "relative"){
counted[[i]][, ncol(counted[[i]])] <- round(counted[[i]][, ncol(counted[[i]]), drop = F]/sum(counted[[i]][, ncol(counted[[i]]), drop = F]), 3)
}
# browser()
colnames(counted[[i]]) <- c("result", colnames(reduced)[i]) # change colnames
ifelse(test = i == 1, yes = final <- counted[[i]][, 1, drop = F], no = final <- final) # for the first iteration, keep the group variable and results column
final <- cbind(final, counted[[i]][, ncol(counted[[i]]), drop = F]) # Add the results for each measure
} else {
# browser()
counted[[i]] <- data %>% group_by(across(all_of(by))) %>% count(get(colnames(reduced)[i]), .drop = F) %>% filter(!is.na(`get(colnames(reduced)[i])`)) # count per measure
# browser()
if(type == "relative"){
counted[[i]] <- counted[[i]] %>% group_by(across(all_of(by))) %>% mutate(prop_n = n / sum(n)) %>% select(!n)
# counted[[i]][, ncol(counted[[i]])] <- round(counted[[i]][, ncol(counted[[i]])]/sum(counted[[i]][1:3, ncol(counted[[i]])],), 3)
}
colnames(counted[[i]]) <- c(by, "result", colnames(reduced)[i]) # change colnames
ifelse(test = i == 1, yes = final <- counted[[i]][, c(seq_len(length(by)), (length(by) + 1))], no = final <- final) # for the first iteration, keep the group variable and results column
final <- cbind(final, counted[[i]][, ncol(counted[[i]]), drop = F]) # Add the results for each measure
}
}
return(final)
}
# Pre-check to know if there are NAs
colSums(apply(Results_final, 2, is.na))
# Main effects
count_results(data = Results_final, by = "total", type = "relative")
K_res   <- count_results(data = Results_final, by = c("nclus"), type = "relative")
N_res   <- count_results(data = Results_final, by = c("N_g"), type = "relative")
G_res   <- count_results(data = Results_final, by = c("ngroups"), type = "relative")
B_res   <- count_results(data = Results_final, by = c("coeff"), type = "relative")
Bal_res <- count_results(data = Results_final, by = c("balance"), type = "relative")
sd_res  <- count_results(data = Results_final, by = c("sd"), type = "relative")
mean(Results_final$entropyR2)
View(Results_final %>% group_by(nclus, N_g, ngroups, coeff, balance, sd) %>% summarise(across(entropyR2, mean)))
# HEATMAP
a <- count_results(data = Results_final, by = c("sd", "nclus"), type = "relative") %>% filter(result == "Correct")
a1 <- a %>% dplyr::select(sd, nclus, Chull:ICL) %>% pivot_longer(cols = Chull:ICL, names_to = "Measure", values_to = "Proportion")
a1$Measure <- factor(a1$Measure, levels = c("BIC_N", "ICL", "BIC_G", "AIC3", "AIC", "Chull"))
plot <- ggplot(data = a1, aes(x = sd, y = Measure)) + facet_grid(~nclus) +
geom_tile(aes(fill = Proportion)) + geom_text(aes(label = Proportion), size = 3.2) +
scale_fill_gradient(low = "yellow", high = "green4") +
scale_x_continuous(sec.axis = sec_axis(~ . , name = "Number of clusters", breaks = NULL, labels = NULL)) +
labs(x = expression("Within-cluster differences (" * sigma[beta] * ")"),  # Combines text with Greek letter, no space
y = expression("Model Selection measure")) +
scale_y_discrete(labels = c("Chull" = "CHull",
"AIC" = "AIC",
"AIC3" = expression(AIC[3]),
"BIC_G" = expression(BIC[G]),
"ICL" = "ICL",
"BIC_N" = expression(BIC[N])))
plot
# HEATMAP
a <- count_results(data = Results_final, by = c("sd", "nclus"), type = "relative") %>% filter(result == "Correct")
a1 <- a %>% dplyr::select(sd, nclus, Chull:ICL) %>% pivot_longer(cols = Chull:ICL, names_to = "Measure", values_to = "Proportion")
a1$Measure <- factor(a1$Measure, levels = c("BIC_N", "ICL", "BIC_G", "AIC3", "AIC", "Chull"))
plot <- ggplot(data = a1, aes(x = sd, y = Measure)) + facet_grid(sd~nclus) +
geom_tile(aes(fill = Proportion)) + geom_text(aes(label = Proportion), size = 3.2) +
scale_fill_gradient(low = "yellow", high = "green4") +
scale_x_continuous(sec.axis = sec_axis(~ . , name = "Number of clusters", breaks = NULL, labels = NULL)) +
labs(x = expression("Within-cluster differences (" * sigma[beta] * ")"),  # Combines text with Greek letter, no space
y = expression("Model Selection measure")) +
scale_y_discrete(labels = c("Chull" = "CHull",
"AIC" = "AIC",
"AIC3" = expression(AIC[3]),
"BIC_G" = expression(BIC[G]),
"ICL" = "ICL",
"BIC_N" = expression(BIC[N])))
plot
# HEATMAP
a <- count_results(data = Results_final, by = c("sd", "nclus"), type = "relative") %>% filter(result == "Correct")
a1 <- a %>% dplyr::select(sd, nclus, Chull:ICL) %>% pivot_longer(cols = Chull:ICL, names_to = "Measure", values_to = "Proportion")
a1$Measure <- factor(a1$Measure, levels = c("BIC_N", "ICL", "BIC_G", "AIC3", "AIC", "Chull"))
plot <- ggplot(data = a1, aes(x = sd, y = Measure)) #+ facet_grid(sd~nclus) +
geom_tile(aes(fill = Proportion)) + geom_text(aes(label = Proportion), size = 3.2) +
scale_fill_gradient(low = "yellow", high = "green4") +
scale_x_continuous(sec.axis = sec_axis(~ . , name = "Number of clusters", breaks = NULL, labels = NULL)) +
labs(x = expression("Within-cluster differences (" * sigma[beta] * ")"),  # Combines text with Greek letter, no space
y = expression("Model Selection measure")) +
scale_y_discrete(labels = c("Chull" = "CHull",
"AIC" = "AIC",
"AIC3" = expression(AIC[3]),
"BIC_G" = expression(BIC[G]),
"ICL" = "ICL",
"BIC_N" = expression(BIC[N])))
plot
plot <- ggplot(data = a1, aes(x = sd, y = Measure)) + #facet_grid(sd~nclus) +
geom_tile(aes(fill = Proportion)) + geom_text(aes(label = Proportion), size = 3.2) +
scale_fill_gradient(low = "yellow", high = "green4") +
scale_x_continuous(sec.axis = sec_axis(~ . , name = "Number of clusters", breaks = NULL, labels = NULL)) +
labs(x = expression("Within-cluster differences (" * sigma[beta] * ")"),  # Combines text with Greek letter, no space
y = expression("Model Selection measure")) +
scale_y_discrete(labels = c("Chull" = "CHull",
"AIC" = "AIC",
"AIC3" = expression(AIC[3]),
"BIC_G" = expression(BIC[G]),
"ICL" = "ICL",
"BIC_N" = expression(BIC[N])))
plot
# HEATMAP
a <- count_results(data = Results_final, by = c("sd", "nclus"), type = "relative") %>% filter(result == "Correct")
View(a)
count_results(data = Results_final, by = c("sd", "N_g"), type = "relative")
count_results(data = Results_final, by = c("sd", "nclus"), type = "relative")
# HEATMAP
a <- count_results(data = Results_final, by = c("sd", "nclus"), type = "relative") %>% filter(result == "0")
View(a)
a1 <- a %>% dplyr::select(sd, nclus, Chull:ICL) %>% pivot_longer(cols = Chull:ICL, names_to = "Measure", values_to = "Proportion")
a1$Measure <- factor(a1$Measure, levels = c("BIC_N", "ICL", "BIC_G", "AIC3", "AIC", "Chull"))
plot <- ggplot(data = a1, aes(x = sd, y = Measure)) + #facet_grid(sd~nclus) +
geom_tile(aes(fill = Proportion)) + geom_text(aes(label = Proportion), size = 3.2) +
scale_fill_gradient(low = "yellow", high = "green4") +
scale_x_continuous(sec.axis = sec_axis(~ . , name = "Number of clusters", breaks = NULL, labels = NULL)) +
labs(x = expression("Within-cluster differences (" * sigma[beta] * ")"),  # Combines text with Greek letter, no space
y = expression("Model Selection measure")) +
scale_y_discrete(labels = c("Chull" = "CHull",
"AIC" = "AIC",
"AIC3" = expression(AIC[3]),
"BIC_G" = expression(BIC[G]),
"ICL" = "ICL",
"BIC_N" = expression(BIC[N])))
plot
# HEATMAP
a <- count_results(data = Results_final, by = c("sd", "N_g"), type = "relative") %>% filter(result == "0")
a1 <- a %>% dplyr::select(sd, N_g, Chull:ICL) %>% pivot_longer(cols = Chull:ICL, names_to = "Measure", values_to = "Proportion")
a1$Measure <- factor(a1$Measure, levels = c("BIC_N", "ICL", "BIC_G", "AIC3", "AIC", "Chull"))
plot <- ggplot(data = a1, aes(x = sd, y = Measure)) + facet_grid(~N_g) +
geom_tile(aes(fill = Proportion)) + geom_text(aes(label = Proportion), size = 3.2) +
scale_fill_gradient(low = "yellow", high = "green4") +
scale_x_continuous(sec.axis = sec_axis(~ . , name = "Number of clusters", breaks = NULL, labels = NULL)) +
labs(x = expression("Within-cluster differences (" * sigma[beta] * ")"),  # Combines text with Greek letter, no space
y = expression("Model Selection measure")) +
scale_y_discrete(labels = c("Chull" = "CHull",
"AIC" = "AIC",
"AIC3" = expression(AIC[3]),
"BIC_G" = expression(BIC[G]),
"ICL" = "ICL",
"BIC_N" = expression(BIC[N])))
plot
# TABLES
K_res   <- count_results(data = Results_final, by = c("nclus"), type = "relative")   %>% select(nclus:ICL)
N_res   <- count_results(data = Results_final, by = c("N_g"), type = "relative")     %>% select(N_g:ICL)
G_res   <- count_results(data = Results_final, by = c("ngroups"), type = "relative") %>% select(ngroups:ICL)
B_res   <- count_results(data = Results_final, by = c("coeff"), type = "relative")   %>% select(coeff:ICL)
Bal_res <- count_results(data = Results_final, by = c("balance"), type = "relative") %>% select(balance:ICL)
sd_res  <- count_results(data = Results_final, by = c("sd"), type = "relative")      %>% select(sd:ICL)
to_res  <- count_results(data = Results_final, by = "total", type = "relative")      %>% select(result:ICL)
K_res   <- K_res %>%
pivot_longer(cols = -c(nclus, result), names_to = "metric", values_to = "value") %>%
pivot_wider(names_from = nclus, values_from = value, names_prefix = "nclus") %>%
relocate(metric) %>% arrange(metric)
N_res   <- N_res %>%
pivot_longer(cols = -c(N_g, result), names_to = "metric", values_to = "value") %>%
pivot_wider(names_from = N_g, values_from = value, names_prefix = "N_g") %>%
relocate(metric) %>% arrange(metric) %>% select(contains("N_g"))
G_res   <- G_res %>%
pivot_longer(cols = -c(ngroups, result), names_to = "metric", values_to = "value") %>%
pivot_wider(names_from = ngroups, values_from = value, names_prefix = "ngroups") %>%
relocate(metric) %>% arrange(metric) %>% select(contains("ngroups"))
B_res   <- B_res %>%
pivot_longer(cols = -c(coeff, result), names_to = "metric", values_to = "value") %>%
pivot_wider(names_from = coeff, values_from = value, names_prefix = "coeff") %>%
relocate(metric) %>% arrange(metric) %>% select(contains("coeff"))
Bal_res <- Bal_res %>%
pivot_longer(cols = -c(balance, result), names_to = "metric", values_to = "value") %>%
pivot_wider(names_from = balance, values_from = value, names_prefix = "balance") %>%
relocate(metric) %>% arrange(metric) %>% select(contains("balance"))
sd_res  <- sd_res %>%
pivot_longer(cols = -c(sd, result), names_to = "metric", values_to = "value") %>%
pivot_wider(names_from = sd, values_from = value, names_prefix = "sd") %>%
relocate(metric) %>% arrange(metric) %>% select(contains("sd"))
to_res  <- to_res %>%
pivot_longer(cols = -c(result), names_to = "metric", values_to = "value") %>%
relocate(metric) %>% arrange(metric) %>% select(value)
final <- cbind(K_res, N_res, G_res, B_res, Bal_res, sd_res, to_res)
# Add totals per column
total_col <- cbind("total", final %>% group_by(result) %>% summarise(across(where(is.numeric), mean, na.rm = TRUE)))
colnames(total_col)[1] <- "metric"; colnames(total_col)[ncol(total_col)] <- "value"
final <- rbind(final, total_col)
View(final)
# Main effects
count_results(data = Results_final, by = "total", type = "relative")
sd_res  <- count_results(data = Results_final, by = c("sd"), type = "relative")
sd_res
HS.model <- ' visual  =~ x1 + x2 + x3
textual =~ x4 + x5 + x6
speed   =~ x7 + x8 + x9 '
fit <- lavaan(HS.model, data=HolzingerSwineford1939, meanstructure = T)
library(lavaan)
HS.model <- ' visual  =~ x1 + x2 + x3
textual =~ x4 + x5 + x6
speed   =~ x7 + x8 + x9 '
fit <- lavaan(HS.model, data=HolzingerSwineford1939, meanstructure = T)
summary(fit, fit.measures=TRUE)
fit <- lavaan(HS.model, data=HolzingerSwineford1939)
HolzingerSwineford1939
fit <- sem(HS.model, data = HolzingerSwineford1939, meanstructure = T)
summary(fit, fit.measures=TRUE)
fit <- sem(HS.model, data = HolzingerSwineford1939, meanstructure = T, group = "school")
summary(fit, fit.measures=TRUE)
lavInspect(fit, what = "est")
